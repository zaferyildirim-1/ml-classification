{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "id": "CvFxPmf41b6y"
   },
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "id": "WZUMKNQY1b6y"
   },
   "source": [
    "Applied EDA processes for the development of predictive models. Handling outliers, domain knowledge and feature engineering will be challenges.\n",
    "\n",
    "This project aims to improve ability to implement algorithms for Multi-Class Classification, implement many algorithms commonly used for Multi-Class Classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "id": "laCRtJs51b6z"
   },
   "source": [
    "# Determines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "id": "iixh9Bej1b6z"
   },
   "source": [
    "The 2012 US Army Anthropometric Survey (ANSUR II) was executed by the Natick Soldier Research, Development and Engineering Center (NSRDEC) from October 2010 to April 2012 and is comprised of personnel representing the total US Army force to include the US Army Active Duty, Reserves, and National Guard. In addition to the **anthropometric and demographic data** described below, the ANSUR II database also consists of **3D whole body, foot, and head scans** of Soldier participants. These 3D data are not publicly available out of respect for the privacy of ANSUR II participants. The data from this survey are used for a wide range of equipment design, sizing, and tariffing applications within the military and has many potential commercial, industrial, and academic applications.\n",
    "\n",
    "The ANSUR II working databases contain **93 anthropometric measurements which were directly measured, and 15 demographic/administrative** variables explained below. The ANSUR II Male working database contains a total sample of 4,082 subjects. The ANSUR II Female working database contains a total sample of 1,986 subjects.\n",
    "\n",
    "\n",
    "DATA DICT:\n",
    "https://data.world/datamil/ansur-ii-data-dictionary/workspace/file?filename=ANSUR+II+Databases+Overview.pds\n",
    "\n",
    "---\n",
    "\n",
    "To achieve high prediction success, you must understand the data well and develop different approaches that can affect the dependent variable.\n",
    "\n",
    "Firstly, try to understand the dataset column by column using pandas module. Do research within the scope of domain (body scales, and race characteristics) knowledge on the internet to get to know the data set in the fastest way.\n",
    "\n",
    "You will implement ***Logistic Regression, Support Vector Machine, XGBoost, Random Forest*** algorithms. Also, evaluate the success of your models with appropriate performance metrics.\n",
    "\n",
    "At the end of the project, choose the most successful model and try to enhance the scores with ***SMOTE*** make it ready to deploy. Furthermore, use ***SHAP*** to explain how the best model you choose works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "id": "P2UckCvP1b60"
   },
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "id": "gCVDEsGB1b60"
   },
   "source": [
    "#### 1. Exploratory Data Analysis (EDA)\n",
    "- Import Libraries, Load Dataset, Exploring Data\n",
    "\n",
    "    *i. Import Libraries*\n",
    "    \n",
    "    *ii. Ingest Data *\n",
    "    \n",
    "    *iii. Explore Data*\n",
    "    \n",
    "    *iv. Outlier Detection*\n",
    "    \n",
    "    *v.  Drop unnecessary features*\n",
    "\n",
    "#### 2. Data Preprocessing\n",
    "- Scale (if needed)\n",
    "- Separete the data frame for evaluation purposes\n",
    "\n",
    "#### 3. Multi-class Classification\n",
    "- Import libraries\n",
    "- Implement SVM Classifer\n",
    "- Implement Decision Tree Classifier\n",
    "- Implement Random Forest Classifer\n",
    "- Implement XGBoost Classifer\n",
    "- Compare The Models\n",
    "\n",
    "#### 4. SMOTE\n",
    "- Apply Imbalance Learning Techniques\n",
    "\n",
    "#### 5. SHAP\n",
    "- Apply Feature selection with SHAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Y5-Kay1Eqvdj"
   },
   "source": [
    "# EDA\n",
    "- Drop unnecessary colums\n",
    "- Drop DODRace class if value counts below 500 (we assume that our data model can't learn if it is below 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fatnG40l-SZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "\n",
    "import xgboost\n",
    "import sklearn\n",
    "import shap\n",
    "import imblearn\n",
    "\n",
    "print('imblearn version:', imblearn.__version__)  # imblearn version : '0.12.4'\n",
    "print(\"scikit-learn version:\", sklearn.__version__) # scikit-learn version: 1.4.0\n",
    "print(\"xgboost version:\", xgboost.__version__) # XGBoost version : 2.1.3\n",
    "print(\"numpy version:\", np.__version__)# numpy version : 1.23.5\n",
    "print(\"shap version:\", shap.__version__)# shao version : 0.41.0\n",
    "print(\"seaborn version:\", sns.__version__) # seaborn version : 0.12.2\n",
    "\n",
    "# These versions must be used together for compatibility, otherwise you will get an error.\n",
    "# i worked on uv environment, took a lot of time to setup these :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7,4)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) read CSV files (latin1 for encoding errors )\n",
    "\n",
    "ds_female = pd.read_csv('fm.csv', encoding='latin1')\n",
    "ds_male   = pd.read_csv('m.csv',   encoding='latin1')\n",
    "\n",
    "# normalize column names\n",
    "\n",
    "ds_female.columns = ds_female.columns.str.lower()\n",
    "ds_male.columns   = ds_male.columns.str.lower()\n",
    "\n",
    "\n",
    "# 3)concat\n",
    "fm = pd.concat([ds_female, ds_male], ignore_index=True)\n",
    "\n",
    "# \n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.shape\n",
    "fm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_f = ds_female.columns.tolist()\n",
    "cols_m = ds_male.columns.tolist()\n",
    "\n",
    "# \n",
    "print(\"col names are same :\", cols_f == cols_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SubjectId\n",
    "fm.drop(columns=['SubjectId', 'subjectid'], inplace=True, errors='ignore')\n",
    "\n",
    "# \n",
    "print(fm.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) fmâ€™in kolon adlarÄ±nÄ±n gerÃ§ekten kÃ¼Ã§Ã¼k harfli olduÄŸundan emin ol\n",
    "print(fm.columns.tolist())\n",
    "\n",
    "# 2) 'dodrace' sÃ¼tununu incele\n",
    "print(\"DODRace deÄŸerleri ve frekanslarÄ±:\")\n",
    "print(fm['dodrace'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nBenzersiz DODRace kategorileri:\")\n",
    "print(fm['dodrace'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm[\"dodrace\"].value_counts().plot(kind=\"pie\", figsize=(6,3) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct = fm.isnull().mean() * 100  \n",
    "cols_to_drop_missing = missing_pct[missing_pct > 50].index.tolist()\n",
    "cols_to_drop_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DODRace kodlarÄ±nÄ± anlamlÄ± Ä±rk etiketlerine dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "\n",
    "# 1) Kodâ€“etiket eÅŸlemesini tanÄ±mla\n",
    "race_mapping = {\n",
    "    1: 'White', \n",
    "    2: 'Black_or_African_American', \n",
    "    3: 'Hispanic', \n",
    "    4: 'Asian', \n",
    "    5: 'Native_American', \n",
    "    6: 'Pacific_Islander', \n",
    "    8: 'Other'\n",
    "}\n",
    "\n",
    "# 2) Yeni bir sÃ¼tun ekleyerek sayÄ±sal kodlarÄ± etiketlere Ã§evir\n",
    "fm['dodrace_label'] = fm['dodrace'].map(race_mapping)\n",
    "\n",
    "# 3) Ä°lk 14 satÄ±rÄ± gÃ¶rÃ¼ntÃ¼leyerek dÃ¶nÃ¼ÅŸÃ¼mÃ¼ doÄŸrula\n",
    "fm.loc[:13, ['dodrace', 'dodrace_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm['dodrace_label'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Her sÃ¼tundaki eksik deÄŸer sayÄ±sÄ±nÄ± ve oranÄ±nÄ± hesapla\n",
    "missing_count = fm.isnull().sum()\n",
    "missing_pct   = fm.isnull().mean() * 100\n",
    "\n",
    "# 2) SonuÃ§larÄ± tek bir DataFrameâ€™de birleÅŸtir ve eksik oranÄ±na gÃ¶re sÄ±rala\n",
    "missing_ds = pd.DataFrame({\n",
    "    'missing_count': missing_count,\n",
    "    'missing_pct': missing_pct\n",
    "}).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "# 3) Eksik veri tablosunun ilk 10 satÄ±rÄ±nÄ± gÃ¶rÃ¼ntÃ¼le\n",
    "print(\"=== Eksik Veri Ã–zeti (En YÃ¼ksek %10) ===\")\n",
    "print(missing_ds.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksik veri oranÄ± %50â€™den fazla olan sÃ¼tunu kaldÄ±rma\n",
    "fm.drop(columns=['ethnicity'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kontrol: artÄ±k eksik yÃ¼zde listesinde gÃ¶rÃ¼nmemeli\n",
    "print(\"GÃ¼ncel eksik oranlarÄ±:\\n\", (fm.isnull().mean() * 100).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find how many unique values object (categorical) features have\n",
    "\n",
    "\n",
    "for col in fm.select_dtypes(\"object\"):  # Iterate over object type columns\n",
    "    print(f\"{col} has {fm[col].nunique()} unique value\")\n",
    "\n",
    "# We check our unique categorical observation numbers.\n",
    "# We will drop the feature (Date), which shows the body measurement dates,\n",
    "# the units where the measurements are done (installation),\n",
    "# the specialty of the soldiers (PrimaryMOS) will not provide insight into races.\n",
    "\n",
    "# We will check below whether the unit (component) where the soldiers are working\n",
    "# and the branch (branch) they are working with have an effect.\n",
    "# (Like blacks with relatively better physical strength come to the fore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"date installation component branch primarymos subjectsbirthlocation writingpreference\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategorik DeÄŸiÅŸken DaÄŸÄ±lÄ±mlarÄ±\n",
    "\n",
    "# 1) Kategorik sÃ¼tunlarÄ± tespit et\n",
    "categorical_cols = fm.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 2) Her bir kategorik sÃ¼tunun frekans daÄŸÄ±lÄ±mÄ±nÄ± yazdÄ±r\n",
    "for col in categorical_cols:\n",
    "    print(f\"=== {col} daÄŸÄ±lÄ±mÄ± ===\")\n",
    "    print(fm[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm['subjectsbirthlocation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.drop(['subjectnumericrace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.groupby(['component'])[\"dodrace\"].value_counts(normalize=True\n",
    "\n",
    "# race and dist by component. \n",
    "# ayirt edici degil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.groupby(['component', \"branch\"])[\"dodrace_label\"].value_counts(normalize=True)\n",
    "\n",
    "# ayirt ediciligi yok / az\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated list of columns to drop \n",
    "drop_list2 = [\n",
    "    \"date\",\n",
    "    \"installation\",\n",
    "    \"component\",\n",
    "    \"branch\",\n",
    "    \"primarymos\",\n",
    "    \"weightlbs\",                # beyan edilen\n",
    "    \"heightin\",                 # ''     ''\n",
    "    \"subjectnumericrace\",\n",
    "]\n",
    "\n",
    "# Drop the selected columns\n",
    "fm.drop(columns=drop_list2, inplace=True)\n",
    "\n",
    "# Notes:\n",
    "# - Dropped columns include identifiers, self-reported values, and potential leakage sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.drop(columns=[\"dodrace\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.dodrace_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.rename(columns={\"dodrace_label\": \"dodrace\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.dodrace.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'dodrace' is one of the selected categories\n",
    "ds = fm[fm[\"dodrace\"].isin([\"White\", \"Black_or_African_American\", \"Hispanic\"])]\n",
    "\n",
    "# Comment:\n",
    "# We are keeping only individuals who are labeled as White, Black, or Hispanic in 'dodrace'.\n",
    "# This subset (ds2) will be used for a focused multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect realistic body proportion anomalies using named columns\n",
    "\n",
    "\n",
    "def detect_body_proportion_anomalies(row):\n",
    "\n",
    "    \"\"\"\n",
    "    Detects anatomical and proportional anomalies in anthropometric data.\n",
    "\n",
    "    This function evaluates a set of domain-informed conditions based on real-world\n",
    "    human body proportions using body measurement features. It highlights rows with\n",
    "    suspicious or implausible values that may result from measurement errors or\n",
    "    data entry mistakes.\n",
    "\n",
    "    Anomalies are returned using color-coded styling for Jupyter Notebook visualization.\n",
    "\n",
    "    Color codes:\n",
    "        - 'red'    : Anatomically impossible (e.g., arm longer than body)\n",
    "        - 'orange' : Proportional mismatch (e.g., leg ratio out of realistic bounds)\n",
    "        - 'yellow' : Medical outlier (e.g., BMI extremely low or high)\n",
    "        - ''       : Normal\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row of anthropometric features from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of style strings (e.g., 'color: red') for each cell in the row.\n",
    "    \"\"\"\n",
    "    results = [''] * len(row)\n",
    "\n",
    "    # ðŸ”´ 1. Arm length >= shoulder height\n",
    "    if row[\"acromionradialelength\"] >= row[\"acromialheight\"]:\n",
    "        return ['color: red'] * len(row)\n",
    "\n",
    "    # ðŸŸ  2. Shoulder width vs foot length or ankle circumference anomaly\n",
    "    if row[\"biacromialbreadth\"] > (row[\"balloffootlength\"] + (row[\"balloffootlength\"] - row[\"axillaheight\"]) * 1.5) or \\\n",
    "       row[\"anklecircumference\"] < (row[\"axillaheight\"] - (row[\"balloffootlength\"] - row[\"axillaheight\"])):\n",
    "        return ['color: orange'] * len(row)\n",
    "\n",
    "    # ðŸ”´ 3. Arm span (wingspan) > height by excessive amount (short person with long reach)\n",
    "    if row[\"stature\"] < 150 and row[\"span\"] > 180:\n",
    "        return ['color: red'] * len(row)\n",
    "\n",
    "    # ðŸ”´ 4. Total arm length (shoulder to hand) > body height\n",
    "    total_arm = row[\"shoulderelbowlength\"] + row[\"shoulderlength\"] + row[\"handlength\"]\n",
    "    if total_arm > row[\"stature\"]:\n",
    "        return ['color: red'] * len(row)\n",
    "\n",
    "    # ðŸŸ  5. Leg/stature ratio too small or too large\n",
    "    leg_ratio = row[\"functionalleglength\"] / row[\"stature\"]\n",
    "    if leg_ratio < 0.3 or leg_ratio > 0.55:\n",
    "        return ['color: orange'] * len(row)\n",
    "\n",
    "    # ðŸŸ  6. Head circumference too small or large\n",
    "    if row[\"headcircumference\"] < 48 or row[\"headcircumference\"] > 65:\n",
    "        return ['color: orange'] * len(row)\n",
    "\n",
    "    # ðŸ”´ 7. Waist > buttock circumference (unusual anatomy)\n",
    "    if row[\"waistcircumference\"] > row[\"buttockcircumference\"]:\n",
    "        return ['color: red'] * len(row)\n",
    "\n",
    "    # ðŸŸ¡ 8. BMI too extreme (underweight or obese)\n",
    "    bmi = row[\"weightkg\"] / ((row[\"stature\"] / 100) ** 2)\n",
    "    if bmi < 16 or bmi > 40:\n",
    "        return ['color: yellow'] * len(row)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.style.apply(detect_body_proportion_anomalies, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SayÄ±sal kolonlarÄ± filtrele (NaN olmayan ve gerÃ§ekten sayÄ±sal olanlar)\n",
    "numerical_cols = [\n",
    "    col for col in ds.select_dtypes(include='number').columns\n",
    "    if ds[col].dropna().shape[0] > 0 and pd.api.types.is_numeric_dtype(ds[col])\n",
    "]\n",
    "\n",
    "# Grid ayarÄ±\n",
    "n_cols = 3\n",
    "n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "# Grafik boyutu\n",
    "plt.figure(figsize=(18, n_rows * 5))\n",
    "\n",
    "# Her bir sayÄ±sal deÄŸiÅŸken iÃ§in boxplot (DODRace'e gÃ¶re)\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.boxplot(data=ds, x=\"dodrace\", y=col, palette=\"Set2\")\n",
    "    plt.title(f\"{col} by DODRace\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive pairwise relationship plot for numeric columns\n",
    "\n",
    "# Function to visualize the joint distribution of two numeric columns by race\n",
    "def column_pair(col1, col2):\n",
    "    sns.jointplot(\n",
    "        data=ds,\n",
    "        x=col1,\n",
    "        y=col2,\n",
    "        kind=\"hist\",                      # You can change to \"kde\", \"scatter\", etc.\n",
    "        hue=\"dodrace\",                    # Colour separation by dodrace\n",
    "        palette='Dark2',\n",
    "        height=5,\n",
    "        marginal_kws={\"bins\": 20}         # Bin count for marginal histograms\n",
    "    )\n",
    "\n",
    "# Select only numeric columns for the dropdown menus\n",
    "cols = ds.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "# Create interactive dropdowns for any numeric column pair\n",
    "interact(column_pair, col1=cols, col2=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix of numerical features\n",
    "corr_matrix = ds.select_dtypes(include='number').corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **multicollinearity is not problem for logistic regression with regularisation and non parametric algorithms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.writingpreference.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Replace 'Either hand' with 'Right hand'\n",
    "ds['writingpreference'] = ds['writingpreference'].replace(\n",
    "    'Either hand (No preference)', 'Right hand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.writingpreference.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.select_dtypes(include=[\"object\", \"category\"]).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABD eyaletlerini Census bÃ¶lgelerine gÃ¶re grupla\n",
    "\n",
    "us_region_map = {\n",
    "    'Northeast': ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont',\n",
    "                  'New Jersey', 'New York', 'Pennsylvania'],\n",
    "    'Midwest': ['Indiana', 'Illinois', 'Michigan', 'Ohio', 'Wisconsin', 'Iowa', 'Kansas', 'Minnesota',\n",
    "                'Missouri', 'Nebraska', 'North Dakota', 'South Dakota'],\n",
    "    'South': ['Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia',\n",
    "              'District of Columbia', 'West Virginia', 'Alabama', 'Kentucky', 'Mississippi', 'Tennessee',\n",
    "              'Arkansas', 'Louisiana', 'Oklahoma', 'Texas'],\n",
    "    'West': ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming',\n",
    "             'Alaska', 'California', 'Hawaii', 'Oregon', 'Washington']\n",
    "}\n",
    "\n",
    "# Eyalet â†’ bÃ¶lge eÅŸlemesi\n",
    "state_to_region = {}\n",
    "for region, states in us_region_map.items():\n",
    "    for state in states:\n",
    "        state_to_region[state] = f\"US_{region}\"\n",
    "\n",
    "# Veride sÄ±k geÃ§en yabancÄ± Ã¼lkeler (value_counts verisine gÃ¶re seÃ§ilmiÅŸti)\n",
    "foreign_specific = ['Germany', 'Puerto Rico', 'Mexico', 'Jamaica']\n",
    "\n",
    "# DÃ¶nÃ¼ÅŸtÃ¼rme fonksiyonu (veri silmeden, sadece kategori Ã¼retir)\n",
    "def map_birth_location(loc):\n",
    "    if loc in state_to_region:\n",
    "        return state_to_region[loc]\n",
    "    elif loc in foreign_specific:\n",
    "        return f\"Foreign_{loc}\"\n",
    "    elif pd.isna(loc):\n",
    "        return \"Missing\"\n",
    "    else:\n",
    "        return \"Foreign_Other\"\n",
    "\n",
    "# Yeni sÃ¼tunu ata (mevcut veri setini deÄŸiÅŸtirmez, sadece sÃ¼tun ekler)\n",
    "ds[\"birth_region_grouped\"] = ds[\"subjectsbirthlocation\"].apply(map_birth_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"birth_region_grouped\"].value_counts(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.subjectsbirthlocation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** subjectsbirthlocation drop ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.drop(columns=[\"subjectsbirthlocation\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "K7UZHtvu1b62"
   },
   "source": [
    "## Import Libraries\n",
    "Besides Numpy and Pandas, you need to import the necessary modules for data visualization, data preprocessing, Model building and tuning.\n",
    "\n",
    "*Note: Check out the course materials.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "OqnRjwHB1b64"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "C5lJeTBu1b65"
   },
   "source": [
    "## Ingest Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "tG5BsWraqX_y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "TMjCTEG51b67"
   },
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "SnlGRPWbrNAj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "CS5-GZy0sl4s"
   },
   "source": [
    "# DATA Preprocessing\n",
    "- In this step we divide our data to X(Features) and y(Target) then ,\n",
    "- To train and evaluation purposes we create train and test sets,\n",
    "- Lastly, scale our data if features not in same scale. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " **we have hispanic as minority class and try to predict this minority class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Separate features (X) and target (y)\n",
    "\n",
    "\n",
    "X = ds.drop(columns=[\"dodrace\"])  # drop target column from features\n",
    "y = ds[\"dodrace\"]                 # define target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=101, stratify=y\n",
    ")\n",
    "\n",
    "#We use stratify=y   to preserve class distribution in both train and test\n",
    "# - random_state     ensures reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of train and test sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fr2wgpvk1b7B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "zfi_NOw0s2fM"
   },
   "source": [
    "# Modelling\n",
    "- Fit the model with train dataset\n",
    "- Get predict from vanilla model on both train and test sets to examine if there is over/underfitting   \n",
    "- Apply GridseachCV for both hyperparemeter tuning and sanity test of our model.\n",
    "- Use hyperparameters that you find from gridsearch and make final prediction and evaluate the result according to chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation metric function to compare model performance on training and test sets\n",
    "def eval_metric(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "   \n",
    "    y_train_pred = model.predict(X_train)  # Predict on training set\n",
    "     \n",
    "\n",
    "    y_pred = model.predict(X_test)     # Predict on test set\n",
    "\n",
    "    # --- Evaluation on Test Set ---\n",
    "    print(\"Test_Set\")\n",
    "    print(confusion_matrix(y_test, y_pred))  # Confusion matrix for test set\n",
    "    print(classification_report(y_test, y_pred))  # Classification report for test set\n",
    "\n",
    "    print() \n",
    "\n",
    "    # --- Evaluation on Train Set ---\n",
    "    print(\"Train_Set\")\n",
    "    print(confusion_matrix(y_train, y_train_pred))  # Confusion matrix for train set\n",
    "    print(classification_report(y_train, y_train_pred))  # Classification report for train set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Categoric columns list\n",
    "\n",
    "cat = X_train.select_dtypes(\"object\").columns\n",
    "cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Column transformer\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),  #\n",
    "    remainder=MinMaxScaler(),\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Pipeline steps\n",
    "\n",
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"log\", LogisticRegression(class_weight=\"balanced\", max_iter=10000, random_state=101)) # data imbalanced bu yuzden balanced\n",
    "]\n",
    "\n",
    "# Pipeline\n",
    "pipe_log_model = Pipeline(steps=operations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "N1cviBuh1b7C"
   },
   "source": [
    "## 1. Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "0rSJ5hxp1b7C"
   },
   "source": [
    "### Vanilla Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbKDDck012BS"
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "pipe_log_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "eval_metric(pipe_log_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "# Recall for Hispanic\n",
    "def recall_Hispanic(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None, labels=[\"Hispanic\"])[0]\n",
    "\n",
    "# Precision for Hispanic\n",
    "def precision_Hispanic(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None, labels=[\"Hispanic\"])[0]\n",
    "\n",
    "# F1-score for Hispanic\n",
    "def f1_Hispanic(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None, labels=[\"Hispanic\"])[0]\n",
    "\n",
    "# [0] eklememizin sebebi, recall_score, precision_score, f1_score gibi fonksiyonlarÄ±n\n",
    "# average=None durumunda array dÃ¶ndÃ¼rmesi ve bizim make_scorerâ€™Ä±n iÃ§inden sadece tek sayÄ±yÄ± almak zorunda olmamÄ±zdÄ±r.\n",
    "\n",
    "\n",
    "# Wrap with make_scorer for use in model evaluation or GridSearchCV\n",
    "\n",
    "f1_Hispanic = make_scorer(f1_Hispanic)\n",
    "precision_Hispanic = make_scorer(precision_Hispanic)\n",
    "recall_Hispanic = make_scorer(recall_Hispanic)\n",
    "\n",
    "#response_method=\"predict\" yalnÄ±zca predict_proba veya decision_function kullanan (Ã¶rneÄŸin ROC AUC gibi) metrikler iÃ§in kullanÄ±lÄ±r.\n",
    "#Ama senin recall_score, precision_score ve f1_score metriklerin zaten predict Ã§Ä±ktÄ±sÄ±yla Ã§alÄ±ÅŸÄ±r. Ekstra belirtmene gerek yoktur.\n",
    "\n",
    "# Scoring dictionary\n",
    "scoring = {\n",
    "    \"precision\": precision_Hispanic,\n",
    "    \"recall\": recall_Hispanic,\n",
    "    \"f1\": f1_Hispanic\n",
    "}\n",
    "\n",
    "# In multiclass data, you can get CV scores based on whatever your target label is.\n",
    "# Again, we have to use the make_scorer function. When the data is multiclass,\n",
    "# the average, and labels parameters must be specified in the make_scorer function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"log\", LogisticRegression(class_weight=\"balanced\", max_iter=10000, random_state=101))\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "# 10-fold cross-validation ile Hispanic'e Ã¶zel metrikleri hesapla\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=scoring,         # scoring sÃ¶zlÃ¼ÄŸÃ¼: f1, recall, precision (Hispanic)\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# SkorlarÄ± DataFrame olarak tut\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "\n",
    "# Sadece test skorlarÄ±nÄ±n ortalamasÄ±nÄ± gÃ¶ster\n",
    "df_scores.mean()[2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**skorlar dusuk iyilestirmek lazim**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "lPelWxsU1b7C"
   },
   "source": [
    "### Logistic Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    \"log__C\": [0.01, 0.1, 0.5, 1, 5, 10],  # Regularization strength\n",
    "    \"log__penalty\": [\"l1\", \"l2\"],         # Regularization type\n",
    "    \"log__solver\": [\"liblinear\", \"saga\"]  # Solver'lar: l1+l2 destekleyen\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pipeline adÄ±mlarÄ±\n",
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"log\", LogisticRegression(\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=10000,\n",
    "        random_state=101),),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "# GridSearchCV kurulumu\n",
    "log_model_grid = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=recall_Hispanic,  # bu skoru iyilestirecek sekilde grid search yap\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model's mean test and train scores\n",
    "pd.DataFrame(log_model_grid.cv_results_).loc[\n",
    "    log_model_grid.best_index_, \n",
    "    [\"mean_test_score\", \"mean_train_score\"]\n",
    "]\n",
    "\n",
    "# for RECALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(log_model_grid, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNZyqeNY15nP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict class probabilities\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced data oldugu icin \n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tahmin edilen olasÄ±lÄ±klarÄ± al\n",
    "y_scores = model.predict_proba(X_test)\n",
    "\n",
    "# SÄ±nÄ±f isimlerini sÄ±rayla al\n",
    "class_names = model.classes_\n",
    "\n",
    "# Hedef sÄ±nÄ±flar\n",
    "target_classes = [\"Hispanic\", \"Black_or_African_American\", \"White\"]\n",
    "\n",
    "# Grafik baÅŸlat\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Her hedef sÄ±nÄ±f iÃ§in PR eÄŸrisi + AUC hesapla ve Ã§iz\n",
    "for cls in target_classes:\n",
    "    cls_index = list(class_names).index(cls)\n",
    "    \n",
    "    # Binarize y_test\n",
    "    y_true_binary = (y_test == cls).astype(int)\n",
    "    \n",
    "    # Precision-Recall hesapla\n",
    "    precision, recall, _ = precision_recall_curve(y_true_binary, y_scores[:, cls_index])\n",
    "    \n",
    "    # AUC (average precision score)\n",
    "    auc_score = average_precision_score(y_true_binary, y_scores[:, cls_index])\n",
    "    \n",
    "    # EÄŸriyi Ã§iz, AUC'yi label'a ekle\n",
    "    plt.plot(recall, precision, label=f\"{cls} (AUC = {auc_score:.2f})\")\n",
    "\n",
    "# GrafiÄŸi dÃ¼zenle\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve with AUC\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pro8T6CM19vX"
   },
   "outputs": [],
   "source": [
    "# We can't use the average_precision_score function with the y_test variable because it's not binary\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "y_test_dummies = pd.get_dummies(y_test).values  # we do that for the sake of the average_precision_score function\n",
    "\n",
    "average_precision_score(y_test_dummies[:, 1], y_pred_proba[:, 1])\n",
    "\n",
    "# Returns 0: black, 1: hispanic, 2: white scores.\n",
    "# We got hispanic scores by specifying 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model_grid.predict(X_test)\n",
    "\n",
    "log_AP = average_precision_score(y_test_dummies[:, 1], y_pred_proba[:, 1])\n",
    "log_f1 = f1_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "log_recall = recall_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "\n",
    "# Since we will compare the scores we got from all models in the table below,\n",
    "# we assign model scores to the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "               **logreg solver LIBLiNEAR  for small dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\"\n",
    "            \n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipelogmodellibl = Pipeline(steps=operations)\n",
    "\n",
    "# Fit the model\n",
    "pipelogmodellibl.fit(X_train, y_train)\n",
    "\n",
    "eval_metric(pipelogmodellibl, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "# Perform cross-validation and collect Hispanic-specific metrics\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train,\n",
    "    scoring=scoring,         # scoring dict: precision, recall, f1 (for \"Hispanic\" class)\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Convert results to DataFrame and calculate mean of test/train scores (excluding fit/time columns)\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "GM0PL5eZ1b7E"
   },
   "source": [
    "## 2. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "P3j_Xk1L1b7E"
   },
   "source": [
    "### Vanilla SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  # SVC (Support Vector Classification)\n",
    "\n",
    "\n",
    "operations_svc = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"svc\", SVC(class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "pipe_svc_model = Pipeline(steps=operations_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the SVC model\n",
    "pipe_svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using custom evaluation function\n",
    "eval_metric(pipe_svc_model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=operations_svc)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=10, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "28y9nWxG1b7E"
   },
   "source": [
    "###  SVC Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dFocgCo1-0Z"
   },
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"svc__C\": [0.01, 0.1, 0.5, 1, 10, 50, 100],                 # C iÃ§in daha geniÅŸ aralÄ±k\n",
    "#     \"svc__gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1],       # gamma iÃ§in log Ã¶lÃ§ekli Ã§eÅŸitlilik\n",
    "#     \"svc__kernel\": [\"rbf\", \"poly\", \"sigmoid\"]                  # farklÄ± kernel seÃ§enekleri\n",
    "# }\n",
    "# bu cok yordu bilgisayari\n",
    "\n",
    "param_grid = {\n",
    "    \"svc__C\": [0.1, 1, 10],                         # Avoid extreme values like 0.01 or 100\n",
    "    \"svc__gamma\": [\"scale\", 0.01, 0.1],             # Keep most effective gamma range\n",
    "    \"svc__kernel\": [\"rbf\", \"poly\"]                  # Drop 'sigmoid' â€“ usually underperforms\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"C ile regularization gÃ¼cÃ¼nÃ¼ test edersin (dÃ¼ÅŸÃ¼k C â†’ daha fazla regularization),\n",
    "\n",
    "gamma ile karar sÄ±nÄ±rlarÄ±nÄ±n ne kadar karmaÅŸÄ±k olabileceÄŸini kontrol edersin,\n",
    "\n",
    "kernel ile doÄŸrusal olmayan yapÄ±larÄ± daha iyi modelleme ihtimali doÄŸar.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "operations_svc = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"svc\", SVC(class_weight=None, random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_svc)\n",
    "\n",
    "svm_model_grid = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=recall_Hispanic,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best cross-validation scores (test/train) from GridSearchCV result\n",
    "pd.DataFrame(svm_model_grid.cv_results_).loc[\n",
    "    svm_model_grid.best_index_, [\"mean_test_score\", \"mean_train_score\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the best SVM model on train and test data\n",
    "eval_metric(svm_model_grid, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binarize y_test to one-hot format\n",
    "y_test_bin = label_binarize(y_test, classes=model.classes_)\n",
    "\n",
    "# Get decision scores from SVC\n",
    "decision_function = model.decision_function(X_test)\n",
    "\n",
    "# decision_function Ã§Ä±ktÄ±sÄ±, her sÄ±nÄ±f iÃ§in margin distance (decision boundary'e uzaklÄ±k) verir.\n",
    "# Bu skorlar, precision_recall_curve fonksiyonuna girdi olarak uygundur, \n",
    "# Ã§Ã¼nkÃ¼ sÄ±ralÄ± skorlara gÃ¶re threshold'lar belirleyerek precision-recall Ã§iftleri oluÅŸturur.\n",
    "\n",
    "\n",
    "# Class names for labeling\n",
    "class_names = model.classes_\n",
    "\n",
    "# Plot setup\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Loop through each class\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], decision_function[:, i])\n",
    "    ap_score = average_precision_score(y_test_bin[:, i], decision_function[:, i])\n",
    "    \n",
    "    # Plot precision-recall curve\n",
    "    plt.plot(recall, precision, label=f\"{class_name} (AP={ap_score:.2f})\")\n",
    "\n",
    "# Plot formatting\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curves for All Classes\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_test_dummies[:,1], decision_function[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for the test set\n",
    "y_pred = svm_model_grid.predict(X_test)\n",
    "\n",
    "# Compute Average Precision (AUC) for the Hispanic class using decision scores\n",
    "svc_AP = average_precision_score(y_test_dummies[:, 1], decision_function[:, 1])\n",
    "\n",
    "# Compute F1-score for the Hispanic class\n",
    "svc_f1 = f1_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "\n",
    "# Compute Recall for the Hispanic class\n",
    "svc_recall = recall_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "bDX_iLIls74C"
   },
   "source": [
    "## 3. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat\n",
    "\n",
    "#dont use one hot encode, instead ordinal for rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define OrdinalEncoder to handle unknown categories by assigning them -1\n",
    "ord_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "# Create a column transformer: encode categorical columns, passthrough numeric ones\n",
    "column_trans = make_column_transformer((ord_enc, cat), remainder=\"passthrough\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "qaTzrT6P1b7G"
   },
   "source": [
    "### Vanilla RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "operations_rf = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"RF_model\", RandomForestClassifier(class_weight=None, random_state=101)),\n",
    "]\n",
    "\n",
    "pipe_model_rf = Pipeline(steps=operations_rf)\n",
    "\n",
    "pipe_model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_model_rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanillali berbat halde :)\n",
    "#ilk hali de cv hali de overfit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvtqL0Qg2CKf"
   },
   "outputs": [],
   "source": [
    "operations_rf = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"RF_model\", RandomForestClassifier(class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_rf)\n",
    "\n",
    "# 5-fold cross-validation using custom Hispanic-focused metrics\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "\n",
    "# Wrap results in a DataFrame and calculate mean scores (skip fit_time and score_time)\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 6))\n",
    "df_scores.mean()[2:]  # Only show scoring metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "MkLAZ_M41b7G"
   },
   "source": [
    "### RF Model GridsearchCV. uzayi dene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    \"RF_model__n_estimators\": [100, 200, 300, 400, 500],  # Number of trees in the forest\n",
    "    \"RF_model__max_depth\": [3, 5, 7, None],         # Maximum depth of the tree\n",
    "    \"RF_model__min_samples_split\": [2, 5, 10],\n",
    "    \"RF_model__max_features\": ['sqrt', 'log2', None]\n",
    "    \n",
    "    # \"RF_model__min_samples_split\": [18, 20, 22],  # (optional) Minimum samples to split a node\n",
    "    # \"RF_model__max_features\": ['auto', None, 15, 20]  # (optional) Number of features considered for split\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline steps: Encoding + Random Forest\n",
    "operations_rf = [\n",
    "    (\"OrdinalEncoder\", column_trans),  # Encoding step\n",
    "    (\"RF_model\", RandomForestClassifier(class_weight=\"balanced\", random_state=101)),  # RF classifier\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the pipeline\n",
    "model = Pipeline(steps=operations_rf)\n",
    "\n",
    "# Grid search with custom recall scorer for the Hispanic class\n",
    "rf_grid_model = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=recall_Hispanic,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best mean test and train scores from GridSearchCV results\n",
    "pd.DataFrame(rf_grid_model.cv_results_).loc[\n",
    "    rf_grid_model.best_index_, [\"mean_test_score\", \"mean_train_score\"]\n",
    "]\n",
    "\n",
    "#scoring=recall_Hispanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpmPr3202EbD"
   },
   "outputs": [],
   "source": [
    "rf_grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(rf_grid_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest bu haliyle cok kotu sonuc verdi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binarize the labels for multiclass precision-recall curves\n",
    "y_test_bin = label_binarize(y_test, classes=rf_grid_model.classes_)  # shape: (n_samples, n_classes)\n",
    "y_score = rf_grid_model.predict_proba(X_test)  # shape: (n_samples, n_classes)\n",
    "\n",
    "n_classes = y_test_bin.shape[1]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "for i in range(n_classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    ap = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    plt.plot(recall, precision, lw=2, color=colors[i],\n",
    "             label=f\"{rf_grid_model.classes_[i]} (AUC = {auc_pr:.2f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve for Each Class\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for X_test\n",
    "y_pred = rf_grid_model.predict(X_test)\n",
    "\n",
    "# Calculate Average Precision Score for Hispanic class (index 1)\n",
    "rf_AP = average_precision_score(y_test_dummies[:, 1], y_pred_proba[:, 1])\n",
    "\n",
    "# Calculate F1-score for Hispanic class\n",
    "rf_f1 = f1_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "\n",
    "# Calculate Recall score for Hispanic class\n",
    "rf_recall = recall_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "PvcPc4V81b7H"
   },
   "source": [
    "## 4. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "MuxxUFoW1b7H"
   },
   "source": [
    "### Vanilla XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "operations_xgb = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"XGB_model\", XGBClassifier(random_state=101, use_label_encoder=False)),\n",
    "]\n",
    "\n",
    "pipe_model_xgb = Pipeline(steps=operations_xgb)\n",
    "\n",
    "# sorting will be same as classification_report.\n",
    "y_train_xgb = y_train.map({\n",
    "    \"Black_or_African_American\": 0,\n",
    "    \"Hispanic\": 1,\n",
    "    \"White\": 2\n",
    "})\n",
    "\n",
    "y_test_xgb = y_test.map({\n",
    "    \"Black_or_African_American\": 0,\n",
    "    \"Hispanic\": 1,\n",
    "    \"White\": 2\n",
    "})\n",
    "\n",
    "# If the target is not numeric in xgb 1.6 and higher versions, it returns an error.\n",
    "# That's why we do the conversion manually.\n",
    "\n",
    "pipe_model_xgb.fit(X_train, y_train_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_model_xgb, X_train, y_train_xgb, X_test, y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights for multi-class targets manually\n",
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight=\"balanced\",  # Automatically compute balanced weights\n",
    "    y=y_train_xgb              # Target values must be numeric for XGBoost\n",
    ")\n",
    "\n",
    "classes_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    \"weights\": classes_weights,  # Computed class/sample weights\n",
    "    \"label\": y_train_xgb         # Corresponding encoded target labels\n",
    "}\n",
    "\n",
    "# Combine weights and labels into a DataFrame for inspection\n",
    "comp = pd.DataFrame(my_dict)\n",
    "\n",
    "# Display the first few rows\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = comp.groupby('label').value_counts()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the XGBoost model using instance-level weights\n",
    "pipe_model_xgb.fit(\n",
    "    X_train,\n",
    "    y_train_xgb,\n",
    "    XGB_model__sample_weight=classes_weights  # Pass instance weights to XGBClassifier step\n",
    ")\n",
    "\n",
    "#XGB_model__sample-weight' (cift __)\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost accepts sample weights per instance, not per class.\n",
    "# So we compute balanced weights and assign them to each sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_model_xgb, X_train, y_train_xgb, X_test, y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfi1aa152HbR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72E3Cmnm2KOE"
   },
   "outputs": [],
   "source": [
    "# Define scoring functions to evaluate model performance specifically on the Hispanic class (label = 1)\n",
    "\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define raw metric functions\n",
    "def recall_Hispanic_raw(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average=None, labels=[1])\n",
    "\n",
    "def precision_Hispanic_raw(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None, labels=[1])\n",
    "\n",
    "def f1_Hispanic_raw(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None, labels=[1])\n",
    "\n",
    "# Wrap them without response_method\n",
    "recall_Hispanic = make_scorer(recall_Hispanic_raw)\n",
    "precision_Hispanic = make_scorer(precision_Hispanic_raw)\n",
    "f1_Hispanic = make_scorer(f1_Hispanic_raw)\n",
    "\n",
    "scoring_xgb = {\n",
    "    \"precision\": precision_Hispanic,\n",
    "    \"recall\":    recall_Hispanic,\n",
    "    \"f1\":        f1_Hispanic\n",
    "}\n",
    "\n",
    "\n",
    "# Note: label = 1 corresponds to \"Hispanic\" after mapping, which is why we use labels=[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline steps for XGBoost classifier\n",
    "operations_xgb = [\n",
    "    (\"OrdinalEncoder\", column_trans),  # Categorical encoding step\n",
    "    (\"XGB_model\", XGBClassifier(random_state=101, use_label_encoder=False)),  # XGBoost classifier\n",
    "]\n",
    "\n",
    "# Create pipeline with defined steps\n",
    "model = Pipeline(steps=operations_xgb)\n",
    "\n",
    "# Perform 5-fold cross-validation with custom scoring metrics and instance-level weights\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train_xgb,                         # Encoded target values (Black:0, Hispanic:1, White:2)\n",
    "    scoring=scoring_xgb,                # scoring_xgb: custom scorers for label 1\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    "    fit_params={\"XGB_model__sample_weight\": classes_weights},  # instance-level sample weights\n",
    ")\n",
    "\n",
    "# Convert results to DataFrame and view average performance metrics\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 6))\n",
    "df_scores.mean()[2:]  # Skip the fit times etc., and display averaged test/train metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(xgb_grid_model, X_train, y_train_xgb, X_test, y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "p3gH5QvE1b7I"
   },
   "source": [
    "### XGBoost Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"XGB_model__n_estimators\": [20, 40],      # __ \n",
    "    \"XGB_model__max_depth\": [1, 2],\n",
    "    \"XGB_model__learning_rate\": [0.03, 0.05],\n",
    "    \"XGB_model__subsample\": [0.8, 1],\n",
    "    \"XGB_model__colsample_bytree\": [0.8, 1],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_xgb = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"XGB_model\", XGBClassifier(random_state=101, use_label_encoder=False)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_xgb)\n",
    "\n",
    "xgb_grid_model = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=recall_Hispanic,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_model.fit(\n",
    "    X_train,\n",
    "    y_train_xgb,\n",
    "    XGB_model__sample_weight=classes_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xgb_grid_model.cv_results_).loc[\n",
    "    xgb_grid_model.best_index_,\n",
    "    [\"mean_test_score\", \"mean_train_score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) get probability estimates for each class\n",
    "y_score = xgb_grid_model.predict_proba(X_test)\n",
    "\n",
    "# 2) binarize the test labels\n",
    "classes = [0,1,2]\n",
    "y_test_bin = label_binarize(y_test_xgb, classes=classes)\n",
    "\n",
    "# 3) compute & plot\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, class_id in enumerate(classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    ap = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall, precision, lw=2,\n",
    "             label=f'class {class_id} (AP = {ap:.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('XGBoost Precisionâ€“Recall Curves by Class')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the true XGBoost test labels to one-hot encoded dummy variables\n",
    "y_test_xgb_dummies = pd.get_dummies(y_test_xgb).values\n",
    "\n",
    "# Compute average precision for class â€œ1â€ using predicted probabilities\n",
    "average_precision_score(y_test_xgb_dummies[:, 1], y_pred_proba[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate class predictions on the test set\n",
    "y_pred = xgb_grid_model.predict(X_test)\n",
    "\n",
    "# Calculate Average Precision (AP) for class â€œ1â€\n",
    "xgb_AP = average_precision_score(y_test_xgb_dummies[:, 1], y_pred_proba[:, 1])\n",
    "\n",
    "# Calculate F1 score for class â€œ1â€\n",
    "xgb_f1 = f1_score(y_test_xgb, y_pred, average=None, labels=[1])\n",
    "\n",
    "# Calculate recall for class â€œ1â€\n",
    "xgb_recall = recall_score(y_test_xgb, y_pred, average=None, labels=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "DbXAmOPVDatl"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9A1B65L7jwp"
   },
   "source": [
    "## Other Evaluation Metrics for Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "T1pPLjpA1b7P"
   },
   "source": [
    "- Evaluation metrics\n",
    "https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutliclass ve imbalanced data icin   genel bi score matthew or cohen\n",
    "#\n",
    "#hangi model daha iyi onu bulmak icin.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Me3OZtQF1b7P",
    "outputId": "6280fddb-b392-40c1-87d3-57d242ce2528"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import matthews_corrcoef\n",
    "# matthews_corrcoef?\n",
    "# matthews_corrcoef(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "P74oLhzK1b7P",
    "outputId": "350b8e9d-dd72-4566-8b11-10526699cd94"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import cohen_kappa_score\n",
    "# cohen_kappa_score?\n",
    "# cohen_kappa_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Compute Matthews Correlation Coefficient\n",
    "# ----------------------------------------\n",
    "\n",
    "# Import the metric for balanced evaluation on imbalanced data\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = xgb_grid_model.predict(X_test)\n",
    "\n",
    "# Calculate Matthews correlation coefficient (ranges from â€“1 (total disagreement) to +1 (perfect prediction))\n",
    "mcc = matthews_corrcoef(y_test_xgb, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAU8iMbaa8ae"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Compute Cohenâ€™s Kappa Score for chance-adjusted agreement\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Import Cohen's kappa metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate Cohen's kappa (1 = perfect agreement, 0 = no agreement beyond chance)\n",
    "kappa = cohen_kappa_score(y_test_xgb, y_pred)\n",
    "print(f\"Cohenâ€™s Kappa Score: {kappa:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Compare performance metrics across all models\n",
    "# ===============================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Assemble a DataFrame with F1, Recall, and Average Precision for each model\n",
    "compare = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"SVM\", \"Random Forest\", \"XGBoost\"],\n",
    "    \"F1\":       [log_f1[0],         svc_f1[0],       rf_f1[0],        xgb_f1[0]],\n",
    "    \"Recall\":   [log_recall[0],     svc_recall[0],   rf_recall[0],    xgb_recall[0]],\n",
    "    \"AP\":       [log_AP,            svc_AP,          rf_AP,           xgb_AP]\n",
    "})\n",
    "\n",
    "# 2. Set up a larger figure for three subplots\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# --- Subplot 1: F1 Scores ---\n",
    "plt.subplot(3, 1, 1)\n",
    "f1_sorted = compare.sort_values(by=\"F1\", ascending=False)  # sort by F1 descending\n",
    "ax = sns.barplot(x=\"F1\", y=\"Model\", data=f1_sorted, palette=\"Blues_d\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.3f\")                 # annotate bars with three decimals\n",
    "plt.title(\"Model Comparison: F1 Score\")\n",
    "\n",
    "# --- Subplot 2: Recall Scores ---\n",
    "plt.subplot(3, 1, 2)\n",
    "recall_sorted = compare.sort_values(by=\"Recall\", ascending=False)  # sort by Recall descending\n",
    "ax = sns.barplot(x=\"Recall\", y=\"Model\", data=recall_sorted, palette=\"Blues_d\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.3f\")\n",
    "plt.title(\"Model Comparison: Recall\")\n",
    "\n",
    "# --- Subplot 3: Average Precision (AP) ---\n",
    "plt.subplot(3, 1, 3)\n",
    "ap_sorted = compare.sort_values(by=\"AP\", ascending=False)  # sort by AP descending\n",
    "ax = sns.barplot(x=\"AP\", y=\"Model\", data=ap_sorted, palette=\"Blues_d\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.3f\")\n",
    "plt.title(\"Model Comparison: Average Precision (AP)\")\n",
    "\n",
    "# 3. Improve layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bu data icin logreg iyi, tree based modeller coktu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "9hxUcvZG1b7J"
   },
   "source": [
    "## Before the Deployment\n",
    "- Choose the model that works best based on your chosen metric\n",
    "- For final step, fit the best model with whole dataset to get better performance.\n",
    "- And your model ready to deploy, dump your model and scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------\n",
    "#  Define the column transformer\n",
    "# ----------------------------------------\n",
    "# 'cat' should be a list of your categorical feature names or indices\n",
    "# OneHotEncoder will handle unknown categories by ignoring them,\n",
    "# and remainder=MinMaxScaler() scales all other (numeric) features.\n",
    "\n",
    "column_trans_final = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# . Build the pipeline\n",
    "# ----------------------------------------\n",
    "# First step applies the preprocessing transformer,\n",
    "# second step fits a logistic regression with balanced class weights.\n",
    "operations_final = [\n",
    "    (\"preprocessor\", column_trans_final),\n",
    "    (\n",
    "        \"logistic\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",   # adjust for class imbalance\n",
    "            max_iter=10000,            # ensure convergence\n",
    "            random_state=101           # for reproducibility\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "final_model = Pipeline(steps=operations_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature matrix, y = target array\n",
    "final_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_mean_human = X[X['gender'] == \"Male\"] \\\n",
    "    .describe(include=\"all\") \\\n",
    "    .loc[\"mean\"]\n",
    "male_mean_human\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert that Series into a dict of numeric feature means\n",
    "numeric_means = male_mean_human.drop(labels=['gender', 'writingpreference', 'birth_region_grouped']).to_dict()\n",
    "\n",
    "# 3. Define placeholder values for your 3 categorical columns\n",
    "#    â€“ pick valid entries that appear in your training data\n",
    "cat_values = {\n",
    "    'gender': 'Male',\n",
    "    'writingpreference': 'Right',         # e.g. â€œRightâ€, â€œLeftâ€, or your actual categories\n",
    "    'birth_region_grouped': 'North America'  # e.g. one of the grouped regions\n",
    "}\n",
    "\n",
    "# 4. Merge the numeric means and categorical placeholders\n",
    "example_dict = {**numeric_means, **cat_values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a oneâ€row DataFrame from that dict\n",
    "example_row = pd.DataFrame([example_dict])\n",
    "\n",
    "example_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feed into your trained pipeline\n",
    "prediction = final_model.predict(example_row)\n",
    "print(\"\\nPredicted class for this synthesized example:\", prediction[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "WvWpInu21b7L"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "xg2k1ScZ1b7L"
   },
   "source": [
    "# SMOTE\n",
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "9Rqk02x61b7L"
   },
   "source": [
    "##  Smote implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "XTN4iO7i1b7L"
   },
   "outputs": [],
   "source": [
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Cv5155AN1b7L"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import pipeline as imbpipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1FSKgtbaylV"
   },
   "outputs": [],
   "source": [
    "### en son care basvurulmali cunku yapaylik katiyor\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# 2. Define your preprocessing transformer\n",
    "# ----------------------------------------\n",
    "# 'cat' should be a list of your categorical feature names\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3. Fit-transform the training features\n",
    "# ----------------------------------------\n",
    "# X_train = original training feature matrix\n",
    "X_train_ohe = column_trans.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# 4. Apply SMOTE to balance classes\n",
    "# ----------------------------------------\n",
    "# Initialize SMOTE (you can set random_state for reproducibility)\n",
    "over = SMOTE(random_state=42)\n",
    "# Fit SMOTE on the preprocessed training data\n",
    "X_train_over, y_train_over = over.fit_resample(X_train_ohe, y_train)\n",
    "\n",
    "# Now X_train_over and y_train_over are balanced and ready for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_over.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize the sampler\n",
    "# ----------------------------------------\n",
    "under = RandomUnderSampler(random_state=42)  # fix seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3. Apply under-sampling to your preprocessed training set\n",
    "#    X_train_ohe (or X_train_one) is the one-hot-encoded + scaled matrix\n",
    "#    y_train is the original target array\n",
    "# ----------------------------------------\n",
    "X_train_under, y_train_under = under.fit_resample(X_train_ohe, y_train)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. inspect the new shapes and class balance\n",
    "# ----------------------------------------\n",
    "print(\"Resampled X shape:\", X_train_under.shape)\n",
    "print(\"Resampled y distribution:\\n\", pd.Series(y_train_under).value_counts())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "1hBIqmFL1b7O"
   },
   "source": [
    "## Logistic Regression Over/ Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12AItu1M2ds0"
   },
   "outputs": [],
   "source": [
    "# 2. Configure custom sampling targets\n",
    "# ----------------------------------------\n",
    "# Upsample \"Hispanic\" to 1000 total samples\n",
    "over = SMOTE(\n",
    "    sampling_strategy={'Hispanic': 1000},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Downsample \"White\" to 2500 total samples\n",
    "under = RandomUnderSampler(\n",
    "    sampling_strategy={'White': 2500},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. Apply SMOTE first\n",
    "#    (on your one-hotâ€encoded & scaled training data)\n",
    "# ----------------------------------------\n",
    "X_resampled_over, y_resampled_over = over.fit_resample(X_train_ohe, y_train)\n",
    "\n",
    "\n",
    "y_resampled_over.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 4. Then apply random under-sampling\n",
    "# ----------------------------------------\n",
    "X_resampled_under, y_resampled_under = under.fit_resample(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled_under.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "\n",
    "# 1. Define the sequence of resampling steps:\n",
    "#    - 'o': apply SMOTE to upsample the \"Hispanic\" class\n",
    "#    - 'u': apply RandomUnderSampler to downsample the \"White\" class\n",
    "steps = [\n",
    "    ('o', over),    # SMOTE(sampling_strategy={'Hispanic':1000})\n",
    "    ('u', under)    # RandomUnderSampler(sampling_strategy={'White':2500})\n",
    "]\n",
    "\n",
    "\n",
    "# 2. Build an imblearn Pipeline with those steps\n",
    "pipeline = imbpipeline(steps=steps)\n",
    "\n",
    "# 3. Fit & resample in one go on your preprocessed training data\n",
    "#    X_train_ohe: oneâ€hot encoded & scaled features\n",
    "#    y_train: original target array\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train_ohe, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Verify the new class distribution\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler()\n",
    ")\n",
    "\n",
    "\n",
    "operations = [\n",
    "    (\"preprocessor\", column_trans),\n",
    "    (\"o\", over),\n",
    "    (\"u\", under),\n",
    "    (\"log\", LogisticRegression(max_iter=10000, random_state=101))\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3. Build and fit the imblearn Pipeline\n",
    "# ----------------------------------------\n",
    "smote_pipeline = imbpipeline(steps=operations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit to the raw X_train, y_train in one go:\n",
    "smote_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(smote_pipeline, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = imbpipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=10, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0RF61YIXdIl"
   },
   "source": [
    "#  SHAP\n",
    "- http://archive.today/2024.02.04-155206/https://towardsdatascience.com/shapley-values-clearly-explained-a7f7ef22b104\n",
    "- https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMzSliptXcNM"
   },
   "outputs": [],
   "source": [
    "# Prepare data for SHAP explanations\n",
    "# ----------------------------------------\n",
    "column_trans_shap = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Transform train and test sets manually\n",
    "X_train_trans = column_trans_shap.fit_transform(X_train)\n",
    "X_test_trans  = column_trans_shap.transform(X_test)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Fit a logistic regression model for SHAP\n",
    "# ----------------------------------------\n",
    "model_shap = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=10000,\n",
    "    random_state=101,\n",
    "    penalty=\"l1\",        # l1 lasso feature selection yapacagimiz icin,  \n",
    "    solver=\"saga\",       # baska da\n",
    ")\n",
    "\n",
    "model_shap.fit(X_train_trans, y_train)\n",
    "\n",
    "# Since SHAP doesn't work with the model fitted inside the pipeline,\n",
    "# we apply transformations manually before explaining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(model_shap, X_train_trans, y_train, X_test_trans, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the steps for the pipeline\n",
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans_shap),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"saga\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Build the pipeline\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=scoring,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Aggregate results into a DataFrame and average the test metrics\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= column_trans_shap.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP for feature selection.   (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "# 1. Create a Linear SHAP explainer using the manuallyâ€fit logistic model\n",
    "explainer = shap.LinearExplainer(model_shap, X_train_trans)\n",
    "\n",
    "# 2. Compute SHAP values on the training set\n",
    "shap_values = explainer.shap_values(X_train_trans)\n",
    "\n",
    "# 3. Plot a SHAP summary of feature importances\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    max_display=300,           # show up to 300 features\n",
    "    feature_names=features,    # list of original feature names\n",
    "    plot_size=(20, 100)        # width x height in inches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# 1. Define your class names in the correct order\n",
    "class_names = [\"White\", \"Black\", \"Hispanic\"]\n",
    "\n",
    "# 2. Generate a SHAP barâ€summary plot that shows mean(|SHAP|) per feature,\n",
    "#    split by class (one color per class)\n",
    "shap.summary_plot(\n",
    "    shap_values,                # list of arrays, one (n_samples, n_features) per class\n",
    "    X_train_trans,              # your transformed train set (all numeric)\n",
    "    feature_names=features,     # list of original feature names\n",
    "    class_names=class_names,    # names for each of the three classes\n",
    "    plot_type=\"bar\",            # bar chart of mean absolute SHAP values\n",
    "    max_display=300,             # show top 30 features\n",
    "    plot_size=(15, 35)           # width, height in inches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "green_features = [\n",
    "    \"bideltoidbreadth\",\n",
    "    \"birth_region_grouped\",\n",
    "    \"handlength\",\n",
    "    \"waistdepth\",\n",
    "    \"bimalleolarbreadth\",\n",
    "    \"wristcircumference\",\n",
    "    \"age\",\n",
    "    \"earlength\",\n",
    "    \"bitragionsubmandibulararc\",\n",
    "    \"crotchheight\",\n",
    "    \"forearmcircumferenceflexed\",\n",
    "    \"headlength\",\n",
    "     \"buttockkneelength\",\n",
    "    \"footbreadthhorizontal\",\n",
    "    \"elbowrestheight\",\n",
    "    \"tragiontopofhead\",\n",
    "    \"kneeheightmidpatella\",\n",
    "    \"earprotrusion\",\n",
    "    \"mentonsellionlength\",\n",
    "    \"bizygomaticbreadth\",\n",
    "    \"neckcircumference\",\n",
    "    \"poplitealheight\",\n",
    "  \n",
    "    \"writingpreference\",\n",
    "    \"biacromialbreadth\",\n",
    "\n",
    "    \"crotchlengthomphalion\",\n",
    "    \"earbreadth\",\n",
    "  \n",
    "    \"functionalleglength\",\n",
    "\n",
    "    \"shoulderlength\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X[green_features]\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Identify duplicate column names\n",
    "cols = X2.columns\n",
    "dup_cols = cols[cols.duplicated()]\n",
    "print(\"Duplicate column names found:\", dup_cols.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Drop all but the first occurrence of each duplicate column\n",
    "#    This will keep the first and remove subsequent columns with the same name\n",
    "X2_dedup = X2.loc[:, ~X2.columns.duplicated()]\n",
    "\n",
    "# 3. Verify that duplicates are gone\n",
    "print(\"Columns after deduplication:\", X2_dedup.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_new = X2.select_dtypes(\"object\").columns\n",
    "cat_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Split your selected features and target for SHAP modeling\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2, y, test_size=0.2, random_state=101, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Build a column transformer for SHAP (oneâ€hot + scaling)\n",
    "column_trans_shap = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_new),\n",
    "    remainder=MinMaxScaler(),\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# 3. Define the operations for the SHAP pipeline\n",
    "operations_shap = [\n",
    "    (\"OneHotEncoder\", column_trans_shap),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"saga\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 4. Create the SHAP pipeline\n",
    "pipe_shap_model = Pipeline(steps=operations_shap)\n",
    "pipe_shap_model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_shap_model, X_train2, y_train2, X_test2, y_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate the SHAPâ€based logistic pipeline\n",
    "model = Pipeline(steps=operations_shap)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_train2,\n",
    "    y_train2,\n",
    "    scoring=scoring,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 6))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted class probabilities on the test set\n",
    "y_pred_proba = pipe_shap_model.predict_proba(X_test2)\n",
    "\n",
    "# Plot the precisionâ€“recall curves for each class\n",
    "plot_precision_recall(y_test2, y_pred_proba)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     shap ile feature selection yapmis olduk.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  bunlar(selected features) logreg icin iyi calisir diger modellerde ayni olmaz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "K7UZHtvu1b62",
    "zfi_NOw0s2fM",
    "N1cviBuh1b7C",
    "GM0PL5eZ1b7E",
    "xg2k1ScZ1b7L"
   ],
   "provenance": []
  },
  "hide_input": false,
  "interpreter": {
   "hash": "e4e90950cb561445fc7289d5187c528b28750a487d008a70b474c773afaf79b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 527,
   "position": {
    "height": "40px",
    "left": "1034px",
    "right": "20px",
    "top": "185px",
    "width": "661px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
